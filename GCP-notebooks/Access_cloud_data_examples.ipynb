{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access cloud data examples tutorial\n",
    "\n",
    "- Funding: Interagency Implementation and Advanced Concepts Team [IMPACT](https://earthdata.nasa.gov/esds/impact) for the Earth Science Data Systems (ESDS) program and AWS Public Dataset Program\n",
    "  \n",
    "### Credits: Tutorial development\n",
    "* [Dr. Chelle Gentemann](mailto:gentemann@faralloninstitute.org) -  [Twitter](https://twitter.com/ChelleGentemann)   - Farallon Institute\n",
    "* [Dr. Ryan Abernathey](mailto:rpa@ldeo.columbia.edu) - [Twitter](https://twitter.com/rabernat) - LDEO\n",
    "* [Henri Drake](mailto:hdrake@mit.edu) - [Twitter](https://twitter.com/henrifdrake) - MIT \n",
    "\n",
    "Credits: Tutorial review and comments.\n",
    "* [Dr. Ed Armstrong](mailto:edward.m.armstrong@jpl.nasa.gov) - JPL PODAAC\n",
    "\n",
    "\n",
    "### Data proximate computing: These are BIG datasets that you can analyze on the cloud without downloading the data. \n",
    "\n",
    "\n",
    "### Here we will demonstrate some ways to access the:\n",
    "- Pangeo CMIP6 (climate models)\n",
    "- Pangeo AVISO sea level and current data\n",
    "- AWS MUR sea surface temperatures\n",
    "\n",
    "\n",
    "\n",
    "### To run this notebook\n",
    "\n",
    "Code is in the cells that have <span style=\"color: blue;\">In [  ]:</span> to the left of the cell and have a colored background\n",
    "\n",
    "To run the code:\n",
    "- option 1) click anywhere in the cell, then hold shift and press Enter\n",
    "- option 2) click on the Run button at the top of the page in the dashboard\n",
    "\n",
    "### First start by importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# filter some warning messages\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import intake\n",
    "import dask\n",
    "\n",
    "xr.set_options(display_style=\"html\")  #display dataset nicely \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in AVISO sea surface height data using intake from the Pangeo datastore on Google Cloud\n",
    "\n",
    "\n",
    "For this example we will use gridded [sea-surface altimetry data](\"http://marine.copernicus.eu/services-portfolio/access-to-products/?option=com_csw&view=details&product_id=SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047) from The Copernicus Marine Environment:,\n",
    "\n",
    "This is a widely used dataset in physical oceanography and climate.\n",
    "    \n",
    "![globe image](http://marine.copernicus.eu/documents/IMG/SEALEVEL_GLO_SLA_MAP_L4_REP_OBSERVATIONS_008_027.png)\n",
    "    \n",
    "\n",
    "The dataset has already been extracted from copernicus and stored in google cloud storage in [xarray-zarr](http://xarray.pydata.org/en/latest/io.html#zarr) format.\n",
    "\n",
    "\n",
    "\n",
    "### Here we use the ``intake`` library to access the Pangeo datastore on google cloud\n",
    "\n",
    "This call reads the metadata into memory but in a 'lazy' way.  You haven't actually touched the data yet which is why it is so fast.  Lazy loading means that no computation is performed until you actually ask values to be computed (more [here](http://xarray.pydata.org/en/stable/dask.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cat_pangeo = intake.open_catalog(\"https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/master.yaml\")\n",
    "ds_aviso = cat_pangeo.ocean.sea_surface_height.to_dask()\n",
    "\n",
    "# the number of GB involved in the reduction\n",
    "print(ds_aviso.nbytes/1e9)\n",
    "\n",
    "ds_aviso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "We are looking at the sea level anomaly in this plot and use ``.sel`` to select day to look at.  One of the nice features of ``xarray`` is that it allows you to use coordinates to select data in a way that makes it easy to understand later what you were trying to do.\n",
    "\n",
    "``xarray`` plotting functions rely on matplotlib internally, but they make use of all available metadata to make the plotting operations more intuitive and interpretable. More plotting examples are given [here](http://xarray.pydata.org/en/stable/plotting.html)\n",
    "\n",
    "Here we use [.plot](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.plot.html) method to create a figure of the data with several arguments to specify details in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_aviso['sla'].sel(time='2015-01-01').plot(vmax=1, cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a cluster, a group of computers that will work together.\n",
    "\n",
    "(A cluster is the key to big data analysis on on Cloud.)\n",
    "\n",
    "- This will set up a [dask kubernetes](https://docs.dask.org/en/latest/setup/kubernetes.html) cluster for your analysis and give you a path that you can paste into the top of the Dask dashboard to visualize parts of your cluster.  \n",
    "- You don't need to paste the link below into the Dask dashboard for this to work, but it will help you visualize progress.\n",
    "- Try 20 workers to start (during the tutorial) but you can increase to speed things up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = Gateway()\n",
    "cluster = gateway.new_cluster()\n",
    "cluster.adapt(minimum=1, maximum=20)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ☝️ Don’t forget to click the link above or copy it to the Dask dashboard on the left to view the scheduler dashboard! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a timeseries\n",
    "\n",
    "#### Here we make a simple yet fundamental calculation: the rate of increase of global mean sea level over the observational period.\n",
    "\n",
    "Xarray has a lot of nice build-in methods, such as [.resampe](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.resample.html#xarray-dataset-resample) which can upsample or downsample data and [.mean](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.mean.html#xarray-dataarray-mean). Here we use these to calculate monthly means.  We then take the monthly means and use ``.mean`` again, with the coordinates `latitude` and `longitude` as arguments to create a timeseries of the monthly mean.  Finally, we plot the data and label the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#create the data\n",
    "sla_monthly = ds_aviso['sla'].resample(time='1MS').mean()\n",
    "\n",
    "sla_monthly_timeseries = sla_monthly.mean({'latitude','longitude'})\n",
    "\n",
    "#plot the data\n",
    "sla_monthly_timeseries.plot(label='Full data')\n",
    "plt.ylabel('Sea Level Anomaly [m]')\n",
    "plt.title('Global Mean Sea Level')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CMIP6 data from Google Cloud using intake\n",
    "\n",
    "<img src=\"https://www.carbonbrief.org/wp-content/uploads/2019/11/cmip6_mips.jpg\" alt=\"drawing\" width=300>\n",
    "\n",
    "The CMIP6 data is a huge collection of different experiements.  Access to these data uses the [intake-esm](https://github.com/NCAR/intake-esm) library which you then use the catalog to select specific variables, experiments, or activities. (**Note: intake-esm is quite news and experimental.**)  There are some great tutorials [here](https://github.com/hdrake/cmip6-temperature-demo/) and [here](https://github.com/pangeo-data/pangeo-cmip6-examples/).\n",
    "\n",
    "More information on CMIP6 is [here](https://www.earthsystemcog.org/projects/wip/CMIP6DataRequest) and variable names [here](https://docs.google.com/document/d/1h0r8RZr_f3-8egBMMh7aqLwy3snpD6_MrDz1q8n5XUk/edit)\n",
    "\n",
    "A nice introduction is [here](https://towardsdatascience.com/a-quick-introduction-to-cmip6-e017127a49d3)\n",
    "\n",
    "The Pangeo catalog listing is [here](https://pangeo-data.github.io/pangeo-datastore/cmip6_pangeo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col = intake.open_esm_datastore(\"https://raw.githubusercontent.com/NCAR/intake-esm-datastore/master/catalogs/pangeo-cmip6.json\")\n",
    "\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the collection for historical, monthly, air temperature, for one realization\n",
    "\n",
    "You can use the `variable id` (link given above) to search for different parameters, change the `table id` from atmospheric monthly to ocean monthly, 3hrly data etc.  More information on what you can search for is in this [tutorial](https://intake-esm.readthedocs.io/en/latest/notebooks/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cmip = col.search(experiment_id=['ssp585','historical'],  # pick the `historical` forcing experiment\n",
    "                 table_id='Amon',             # choose to look at atmospheric variables (A) saved at monthly resolution (mon)\n",
    "                 variable_id='tas',           # choose to look at near-surface air temperature (tas) as our variable\n",
    "                 member_id = 'r1i1p1f1')      # arbitrarily pick one realization for each model (i.e. just one set of initial conditions)\n",
    "cat_cmip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data catalog into a dictionary of xarray datasets\n",
    "\n",
    "`dset_dict` is a dictionary of [xarray.Datasets](https://www.carbonbrief.org/wp-content/uploads/2019/11/cmip6_mips.jpg) where its keys refer to compatible groups.\n",
    "\n",
    "We then set the time period we are interested in and create a new dictionary containing the variable of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supress some annoying warnings\n",
    "import logging\n",
    "urllib3_log = logging.getLogger(\"urllib3\")\n",
    "urllib3_log.setLevel(logging.CRITICAL)\n",
    "\n",
    "dset_dict = cat_cmip.to_dataset_dict(zarr_kwargs={'consolidated': True})\n",
    "\n",
    "time_slice = slice('1850','2015') # specific years that bracket our period of interest\n",
    "\n",
    "ds_dict = {}\n",
    "\n",
    "for name, ds in dset_dict.items():\n",
    "    \n",
    "    # rename spatial dimensions if necessary\n",
    "    if ('longitude' in ds.dims) and ('latitude' in ds.dims):\n",
    "        ds = ds.rename({'longitude':'lon', 'latitude': 'lat'}) \n",
    "\n",
    "    #make sure time index is unique, no repeated years....\n",
    "    _, index = np.unique(ds['time'], return_index=True)\n",
    "    ds = ds.isel(time=index)\n",
    "        \n",
    "    ds = ds.sel(time=time_slice) # subset the data for the time period of interest\n",
    "    \n",
    "    # drop redundant coordinates \n",
    "    for coord in ds.coords:\n",
    "        if coord not in ['lat','lon','time']:\n",
    "            ds = ds.drop(coord)\n",
    "    \n",
    "    # Add near-surface air temperature to dictionary\n",
    "    ds_dict[name] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at how air temperatures have changed \n",
    "\n",
    "Look at the data we have, and create means for more recent and historical data to examine how air temperatures have changed.  The last step here is to set a data attribute so that when it is plotted the labels are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temp_diff(ds_cmip, name):\n",
    "    try:\n",
    "        now = ds_cmip.sel(time=slice('2000-01-01','2015-12-31')).mean('time')\n",
    "        then = ds_cmip.sel(time=slice('1850-01-01','1950-12-31')).mean('time')\n",
    "        return (now-then)['tas']\n",
    "    except ValueError as ve:\n",
    "        # some cmip models have weird calendars that cause the above step to fail\n",
    "        print(name, ve)\n",
    "        pass\n",
    "    \n",
    "all_temp_diffs = dask.compute({name: compute_temp_diff(ds_cmip, name)\n",
    "                         for name, ds_cmip in ds_dict.items()})[0]\n",
    "list(all_temp_diffs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'CMIP.NCAR.CESM2-FV2.historical.Amon.gn'\n",
    "temperature_change = all_temp_diffs[model]\n",
    "temperature_change.attrs['long_name'] = 'Change in air temperature (K)'\n",
    "temperature_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the change in temperature\n",
    "\n",
    "Here we use the excellent [cartopy](https://scitools.org.uk/cartopy/docs/latest/) library to plot our data on a projection and add both coastlines and borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = ccrs.Orthographic(-90, 20)           # define target coordinate frame\n",
    "geo = ccrs.PlateCarree()                     # define origin coordinate frame\n",
    "\n",
    "plt.figure(figsize=(9,7))                    #set the figure size\n",
    "ax = plt.subplot(1, 1, 1, projection=ortho)  #create the axis for plotting\n",
    "\n",
    "q = temperature_change.plot(ax=ax, \n",
    "                            transform = geo, \n",
    "                            cmap='OrRd', \n",
    "                            vmin=0, \n",
    "                            vmax=3) # plot a colormap in transformed coordinates\n",
    "\n",
    "ax.add_feature(cartopy.feature.COASTLINE)\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle='-')\n",
    "ax.set_title(f'Global Warming Air Temp - {model}',fontsize=16, ha='center');\n",
    "\n",
    "#plt.savefig('./../../airtemp_warming_patterns.png',dpi=100,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MUR SST](https://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SST) [AWS Public dataset program](https://registry.opendata.aws/mur/) \n",
    "\n",
    "### Access the MUR SST which is in an s3 bucket.  \n",
    "### This Pangeo binder is running on Google Cloud and data access will be slower than running it on AWS.  \n",
    "\n",
    "![image](https://podaac.jpl.nasa.gov/Podaac/thumbnails/MUR-JPL-L4-GLOB-v4.1.jpg)\n",
    "\n",
    "This code is an example of how to read from a s3 bucket.  \n",
    "\n",
    "Right now (2/16/2020) this takes ~1min on AWS and ~2 min on google cloud, there are two issues here and we are working to solve both.  \n",
    "1. In our Zarr datastore the time coodinate is chunked.  We should have this fixed by 3/1/2020.\n",
    "1. Some shortcomings in the s3fs and zarr formats have been identified.  To work on these, git issues were raised to the developers [here](https://github.com/dask/s3fs/issues/285) and [here](https://github.com/zarr-developers/zarr-python/issues/536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_location = 's3://mur-sst/zarr'\n",
    "ds_sst = xr.open_zarr(fsspec.get_mapper(file_location, anon=True),consolidated=True)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read entire 10 years of data at 1 point.\n",
    "\n",
    "Select the ``analysed_sst`` variable over a specific time period, `lat`, and `lon` and load the data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sst_timeseries = ds_sst['analysed_sst'].sel(time=slice('2010-01-01','2020-01-01'),\n",
    "                                            lat=47,\n",
    "                                            lon=-145\n",
    "                                           ).load()\n",
    "sst_timeseries.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anomaly is more interesting...  \n",
    "\n",
    "Use [.groupby](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.groupby.html#xarray-dataarray-groupby) method to calculate the climatology and [.resample](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.resample.html#xarray-dataset-resample) method to then average it into 1-month bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_climatology = sst_timeseries.groupby('time.dayofyear').mean()\n",
    "sst_anomaly = sst_timeseries.groupby('time.dayofyear')-sst_climatology\n",
    "sst_anomaly_monthly = sst_anomaly.resample(time='1MS').mean()\n",
    "\n",
    "#plot the data\n",
    "sst_anomaly.plot()\n",
    "sst_anomaly_monthly.plot()\n",
    "plt.axhline(linewidth=2,color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
